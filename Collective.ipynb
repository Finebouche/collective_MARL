{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2d888a-c6df-48c0-b4f9-e0f854f32264",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2217c171-48ce-4b10-a135-77a635537328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      " 1) rvs/1.0(default)   2) anaconda/3/2021.11   3) cuda/11.4   4) ffmpeg/4.4  \n",
      "\n",
      "Key:\n",
      "(symbolic-version)  \n"
     ]
    }
   ],
   "source": [
    "module(\"unload\", \"cuda/11.6\")\n",
    "module(\"load\", \"cuda/11.4\")\n",
    "module(\"load\",\"ffmpeg\")\n",
    "module(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc86948-b84d-467e-a9d2-d422372d527d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "path_root1 = Path( '/cobra/u/kkumari/warp-drive')\n",
    "path_root2 = Path( '/project_ghent/warp-drive/')\n",
    "sys.path.append(str(path_root1))\n",
    "sys.path.append(str(path_root2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea7f608-2e4f-4eca-86ba-bf4151d8c252",
   "metadata": {},
   "source": [
    "print(sys.path)\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69506557-0774-4887-a043-c60fe9bc88a7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "from animations import (\n",
    "    generate_tag_env_rollout_animation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae374982-1ba9-4db5-aeee-0622ac7aec11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from IPython.display import HTML\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29607d4-55c0-4799-802d-1e620514d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3dea70-9d81-4397-b9c7-d6f05fbe029e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the run config.\n",
    "\n",
    "# Here we show an example configures\n",
    "\n",
    "CFG = \"\"\"\n",
    "# Sample YAML configuration for the tag continuous environment\n",
    "name: \"tag_continuous\"\n",
    "\n",
    "# Environment settings\n",
    "env:\n",
    "    num_preys: 50\n",
    "    num_predators: 5\n",
    "    stage_size: 30\n",
    "    episode_length: 500\n",
    "    preparation_length: 100\n",
    "    max_acceleration: 0.1\n",
    "    max_turn: 2.35  # 3*pi/4 radians\n",
    "    num_acceleration_levels: 10\n",
    "    num_turn_levels: 10\n",
    "    starving_penalty_for_predator: -1.0\n",
    "    surviving_reward_for_prey: 1.0\n",
    "    edge_hit_penalty: -0.1\n",
    "    end_of_game_penalty : -100.0\n",
    "    end_of_game_reward: 100.0\n",
    "    use_full_observation: False\n",
    "    eating_distance: 0.05\n",
    "    seed: 274880\n",
    "    env_backend: \"numba\"\n",
    "\n",
    "# Trainer settings\n",
    "trainer:\n",
    "    num_envs: 400 # number of environment replicas\n",
    "    train_batch_size: 10000 # total batch size used for training per iteration (across all the environments)\n",
    "    num_episodes: 500 # number of episodes to run the training for (can be arbitrarily high)\n",
    "# Policy network settings\n",
    "policy: # list all the policies below\n",
    "    prey:\n",
    "        to_train: True # flag indicating whether the model needs to be trained\n",
    "        algorithm: \"A2C\" # algorithm used to train the policy\n",
    "        gamma: 0.98 # discount rate gamms\n",
    "        lr: 0.005 # learning rate\n",
    "        vf_loss_coeff: 1 # loss coefficient for the value function loss\n",
    "        entropy_coeff:\n",
    "        - [0, 0.5]\n",
    "        - [2000000, 0.05]\n",
    "        model: # policy model settings\n",
    "            module_name: \"fully_connected\" # model type\n",
    "            class_name: \"FullyConnected\" # class type\n",
    "            fc_dims: [256, 256] # dimension(s) of the fully connected layers as a list\n",
    "            model_ckpt_filepath: \"\" # filepath (used to restore a previously saved model)\n",
    "    predator:\n",
    "        to_train: True\n",
    "        algorithm: \"A2C\"\n",
    "        gamma: 0.98\n",
    "        lr: 0.002\n",
    "        vf_loss_coeff: 1\n",
    "        model:\n",
    "            type: \"fully_connected\"\n",
    "            fc_dims: [256, 256]\n",
    "            model_ckpt_filepath: \"\"\n",
    "\n",
    "# Checkpoint saving setting\n",
    "saving:\n",
    "    metrics_log_freq: 100 # how often (in iterations) to print the metrics\n",
    "    model_params_save_freq: 5000 # how often (in iterations) to save the model parameters\n",
    "    basedir: \"/tmp\" # base folder used for saving\n",
    "    name: \"collective_v0\"\n",
    "    tag: \"50preys_1predator\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run_config = yaml.safe_load(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfcdb1a-cf8c-4528-9881-f14adf8f51f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/kkumari/.local/lib/python3.9/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_manager: Setting Numba to use CUDA device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mpcdf/soft/SLE_12/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages/numba/cuda/decorators.py:110: NumbaDeprecationWarning: \u001b[1mEager compilation of device functions is deprecated (this occurs when a signature is provided)\u001b[0m\n",
      "  warn(NumbaDeprecationWarning(msg))\n",
      "/mpcdf/soft/SLE_12/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages/numba/cuda/decorators.py:110: NumbaDeprecationWarning: \u001b[1mEager compilation of device functions is deprecated (this occurs when a signature is provided)\u001b[0m\n",
      "  warn(NumbaDeprecationWarning(msg))\n",
      "/mpcdf/soft/SLE_12/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages/numba/cuda/decorators.py:110: NumbaDeprecationWarning: \u001b[1mEager compilation of device functions is deprecated (this occurs when a signature is provided)\u001b[0m\n",
      "  warn(NumbaDeprecationWarning(msg))\n",
      "/mpcdf/soft/SLE_12/packages/x86_64/anaconda/3/2021.11/lib/python3.9/site-packages/numba/cuda/decorators.py:110: NumbaDeprecationWarning: \u001b[1mEager compilation of device functions is deprecated (this occurs when a signature is provided)\u001b[0m\n",
      "  warn(NumbaDeprecationWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "from custom_env import CUDACustomEnv\n",
    "\n",
    "env_registrar = EnvironmentRegistrar()\n",
    "env_registrar.add_cuda_env_src_path(CUDACustomEnv.name, \"custom_env_step_numba\", env_backend=\"numba\")\n",
    "\n",
    "env_wrapper = EnvWrapper(\n",
    "    env_obj=CUDACustomEnv(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend=\"numba\",\n",
    "    env_registrar=env_registrar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f26aae-eb8a-463e-ba4e-5893152657f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_tag_to_agent_id_map = {\n",
    "    \"predator\": list(env_wrapper.env.predators),\n",
    "    \"prey\": list(env_wrapper.env.preys),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea77e128-653f-4314-9d55-97df4a3f7df3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy module FullyConnected loaded from warp_drive.training.models.fully_connected\n",
      "Policy module FullyConnected loaded from warp_drive.training.models.fully_connected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cobra/u/kkumari/warp-drive/warp_drive/training/trainer.py:249: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seed)\n"
     ]
    }
   ],
   "source": [
    "import warp_drive.training.trainer\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from importlib import reload\n",
    "reload(warp_drive.training.trainer)\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    num_devices=torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cc58683-728f-4ac2-b9b4-0438896a4b56",
   "metadata": {},
   "source": [
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c54822-be12-4932-a31a-64eca7440353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     101.25\n",
      "Mean action sample time per iter (ms)   :      34.82\n",
      "Mean env. step time per iter (ms)       :     259.24\n",
      "Mean training time per iter (ms)        :     118.38\n",
      "Mean total time per iter (ms)           :     524.30\n",
      "Mean steps per sec (policy eval)        :   98769.16\n",
      "Mean steps per sec (action sample)      :  287232.19\n",
      "Mean steps per sec (env. step)          :   38573.58\n",
      "Mean steps per sec (training time)      :   84476.81\n",
      "Mean steps per sec (total)              :   19072.91\n",
      "========================================\n",
      "Metrics for policy 'prey'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.50000\n",
      "Total loss                              : 4861153.00000\n",
      "Policy loss                             : 4804.06982\n",
      "Value function loss                     : 4856351.50000\n",
      "Mean rewards                            :  103.84994\n",
      "Max. rewards                            : 5501.00000\n",
      "Min. rewards                            :    0.90000\n",
      "Mean value function                     :   -0.05035\n",
      "Mean advantages                         : 1002.40302\n",
      "Mean (norm.) advantages                 : 1002.40302\n",
      "Mean (discounted) returns               : 1002.35266\n",
      "Mean normalized returns                 : 1002.35266\n",
      "Mean entropy                            :    4.79253\n",
      "Variance explained by the value function:   -0.00000\n",
      "Std. of action_0 over agents            :    3.19057\n",
      "Std. of action_0 over envs              :    3.19631\n",
      "Std. of action_0 over time              :    3.18327\n",
      "Std. of action_1 over agents            :    3.15068\n",
      "Std. of action_1 over envs              :    3.15710\n",
      "Std. of action_1 over time              :    3.14485\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    : 208753.00405\n",
      "========================================\n",
      "Metrics for policy 'predator'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              : 4327322.50000\n",
      "Policy loss                             : -4289.77246\n",
      "Value function loss                     : 4331612.50000\n",
      "Mean rewards                            :  -92.30002\n",
      "Max. rewards                            :   -1.00000\n",
      "Min. rewards                            : -5501.00000\n",
      "Mean value function                     :   -0.05050\n",
      "Mean advantages                         : -895.54492\n",
      "Mean (norm.) advantages                 : -895.54492\n",
      "Mean (discounted) returns               : -895.59552\n",
      "Mean normalized returns                 : -895.59552\n",
      "Mean entropy                            :    4.79244\n",
      "Variance explained by the value function:    0.00000\n",
      "Std. of action_0 over agents            :    3.05226\n",
      "Std. of action_0 over envs              :    3.16538\n",
      "Std. of action_0 over time              :    3.15591\n",
      "Std. of action_1 over agents            :    3.01905\n",
      "Std. of action_1 over envs              :    3.13462\n",
      "Std. of action_1 over time              :    3.11779\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    : -18537.24696\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/collective_v0/50preys_1predator/1679127078/results.json' \n",
      "[Device 0]: Saving the 'prey' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679127078/prey_10000.state_dict'. \n",
      "[Device 0]: Saving the 'predator' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679127078/predator_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 25 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :      95.56\n",
      "Mean action sample time per iter (ms)   :      33.61\n",
      "Mean env. step time per iter (ms)       :     259.53\n",
      "Mean training time per iter (ms)        :     112.32\n",
      "Mean total time per iter (ms)           :     506.02\n",
      "Mean steps per sec (policy eval)        :  104642.11\n",
      "Mean steps per sec (action sample)      :  297504.53\n",
      "Mean steps per sec (env. step)          :   38531.72\n",
      "Mean steps per sec (training time)      :   89028.72\n",
      "Mean steps per sec (total)              :   19762.07\n",
      "========================================\n",
      "Metrics for policy 'prey'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.44600\n",
      "Total loss                              : 9641409.00000\n",
      "Policy loss                             :    0.00049\n",
      "Value function loss                     : 9641409.00000\n",
      "Mean rewards                            : 2085.49902\n",
      "Max. rewards                            : 5501.00000\n",
      "Min. rewards                            :    0.90000\n",
      "Mean value function                     : 2016.75159\n",
      "Mean advantages                         : 2068.96045\n",
      "Mean (norm.) advantages                 : 2068.96045\n",
      "Mean (discounted) returns               : 4085.71216\n",
      "Mean normalized returns                 : 4085.71216\n",
      "Mean entropy                            :    0.00000\n",
      "Variance explained by the value function:    0.01314\n",
      "Std. of action_0 over agents            :    0.00000\n",
      "Std. of action_0 over envs              :    0.00000\n",
      "Std. of action_0 over time              :    0.00000\n",
      "Std. of action_1 over agents            :    0.00000\n",
      "Std. of action_1 over envs              :    0.00000\n",
      "Std. of action_1 over time              :    0.00000\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.63437\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    : 209735.68039\n",
      "========================================\n",
      "Metrics for policy 'predator'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              : 17062974.00000\n",
      "Policy loss                             :   -0.00897\n",
      "Value function loss                     : 17062974.00000\n",
      "Mean rewards                            : -1754.95007\n",
      "Max. rewards                            :   -1.00000\n",
      "Min. rewards                            : -5501.00000\n",
      "Mean value function                     : -224.98889\n",
      "Mean advantages                         : -3187.28345\n",
      "Mean (norm.) advantages                 : -3187.28345\n",
      "Mean (discounted) returns               : -3412.27197\n",
      "Mean normalized returns                 : -3412.27197\n",
      "Mean entropy                            :    0.00004\n",
      "Variance explained by the value function:    0.00296\n",
      "Std. of action_0 over agents            :    0.00000\n",
      "Std. of action_0 over envs              :    0.00000\n",
      "Std. of action_0 over time              :    0.00000\n",
      "Std. of action_1 over agents            :    0.00000\n",
      "Std. of action_1 over envs              :    0.00000\n",
      "Std. of action_1 over time              :    0.00000\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.65584\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    : -17778.36758\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/collective_v0/50preys_1predator/1679127078/results.json' \n",
      "[Device 0]: Saving the 'prey' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679127078/prey_250000.state_dict'. \n",
      "[Device 0]: Saving the 'predator' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679127078/predator_250000.state_dict'. \n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c77256-da0b-4509-9076-66a36ce7a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Loading the provided trainer model checkpoints. \n",
      "[Device 0]: Loading the 'prey' torch model from the previously saved checkpoint: '/tmp/collective_v0/50preys_1predator/1679127078/prey_250000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'prey' model to 250000. \n",
      "[Device 0]: Loading the 'predator' torch model from the previously saved checkpoint: '/tmp/collective_v0/50preys_1predator/1679127078/predator_250000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'predator' model to 250000. \n"
     ]
    }
   ],
   "source": [
    "trainer.load_model_checkpoint(\n",
    "    {\n",
    "        \"prey\": \"/tmp/collective_v0/50preys_1predator/1679127078/prey_250000.state_dict\",\n",
    "        \"predator\": \"/tmp/collective_v0/50preys_1predator/1679127078/predator_250000.state_dict\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8f4873-c820-4bee-8dff-157e1a7a1b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'cuda_sample_controller'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30638/790665501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the entire episode roll-out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_tag_env_rollout_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cobra/u/kkumari/collective_MARL/animations.py\u001b[0m in \u001b[0;36mgenerate_tag_env_rollout_animation\u001b[0;34m(trainer, fps, tagger_color, runner_color, runner_not_in_game_color, fig_width, fig_height)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Fetch episode states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     episode_states = trainer.fetch_episode_states(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m\"loc_x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loc_y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"still_in_the_game\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;32m/cobra/u/kkumari/warp-drive/warp_drive/training/trainer.py\u001b[0m in \u001b[0;36mfetch_episode_states\u001b[0;34m(self, list_of_states, env_id)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# Sample actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;31m# Step through all the environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cobra/u/kkumari/warp-drive/warp_drive/training/trainer.py\u001b[0m in \u001b[0;36m_sample_actions\u001b[0;34m(self, probabilities, batch_index)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;31m# sample a single or a combined policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_actions_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             actions = self.cuda_envs.cuda_data_manager.data_on_device_via_torch(\n\u001b[1;32m    571\u001b[0m                 \u001b[0m_ACTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cobra/u/kkumari/warp-drive/warp_drive/training/trainer.py\u001b[0m in \u001b[0;36m_sample_actions_helper\u001b[0;34m(self, probabilities, policy_suffix)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction_type_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0maction_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{_ACTIONS}_{action_type_id}\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpolicy_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                 self.cuda_sample_controller.sample(\n\u001b[0m\u001b[1;32m    600\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_data_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'cuda_sample_controller'"
     ]
    }
   ],
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c0c93a-97b6-4658-8d74-68f39d3fa36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Trainer exits gracefully \n"
     ]
    }
   ],
   "source": [
    "# Close the trainer to clear up the CUDA memory heap\n",
    "trainer.graceful_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46149d-695e-4212-9f83-03072ab097d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
