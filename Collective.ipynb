{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2d888a-c6df-48c0-b4f9-e0f854f32264",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d30615-70ee-4ed7-98d9-28098d2580f0",
   "metadata": {},
   "source": [
    "module(\"unload\", \"cuda/11.6\")\n",
    "module(\"load\", \"cuda/11.4\")\n",
    "module(\"load\",\"ffmpeg\")\n",
    "module(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc86948-b84d-467e-a9d2-d422372d527d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "path_root1 = Path( '/cobra/u/kkumari/warp-drive')\n",
    "path_root2 = Path( '/project_ghent/warp-drive/')\n",
    "sys.path.append(str(path_root1))\n",
    "sys.path.append(str(path_root2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea7f608-2e4f-4eca-86ba-bf4151d8c252",
   "metadata": {},
   "source": [
    "print(sys.path)\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69506557-0774-4887-a043-c60fe9bc88a7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "from animations import (\n",
    "    generate_tag_env_rollout_animation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae374982-1ba9-4db5-aeee-0622ac7aec11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from IPython.display import HTML\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29607d4-55c0-4799-802d-1e620514d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3dea70-9d81-4397-b9c7-d6f05fbe029e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the run config.\n",
    "\n",
    "# Here we show an example configures\n",
    "\n",
    "CFG = \"\"\"\n",
    "# Sample YAML configuration for the tag continuous environment\n",
    "name: \"tag_continuous\"\n",
    "\n",
    "# Environment settings\n",
    "env:\n",
    "    num_preys: 50\n",
    "    num_predators: 1\n",
    "    stage_size: 30\n",
    "    episode_length: 500\n",
    "    preparation_length: 100\n",
    "    max_acceleration: 0.1\n",
    "    max_turn: 2.35  # 3*pi/4 radians\n",
    "    num_acceleration_levels: 10\n",
    "    num_turn_levels: 10\n",
    "    starving_penalty_for_predator: -1.0\n",
    "    surviving_reward_for_prey: 1.0\n",
    "    edge_hit_penalty: -0.1\n",
    "    end_of_game_penalty : -100.0\n",
    "    end_of_game_reward: 100.0\n",
    "    use_full_observation: False\n",
    "    eating_distance: 0.05\n",
    "    seed: 274880\n",
    "    env_backend: \"numba\"\n",
    "\n",
    "# Trainer settings\n",
    "trainer:\n",
    "    num_envs: 400 # number of environment replicas\n",
    "    train_batch_size: 10000 # total batch size used for training per iteration (across all the environments)\n",
    "    num_episodes: 500 # number of episodes to run the training for (can be arbitrarily high)\n",
    "# Policy network settings\n",
    "policy: # list all the policies below\n",
    "    prey:\n",
    "        to_train: True # flag indicating whether the model needs to be trained\n",
    "        algorithm: \"A2C\" # algorithm used to train the policy\n",
    "        gamma: 0.98 # discount rate gamms\n",
    "        lr: 0.005 # learning rate\n",
    "        vf_loss_coeff: 1 # loss coefficient for the value function loss\n",
    "        entropy_coeff:\n",
    "        - [0, 0.5]\n",
    "        - [2000000, 0.05]\n",
    "        model: # policy model settings\n",
    "            module_name: \"fully_connected\" # model type\n",
    "            class_name: \"FullyConnected\" # class type\n",
    "            fc_dims: [256, 256] # dimension(s) of the fully connected layers as a list\n",
    "            model_ckpt_filepath: \"\" # filepath (used to restore a previously saved model)\n",
    "    predator:\n",
    "        to_train: True\n",
    "        algorithm: \"A2C\"\n",
    "        gamma: 0.98\n",
    "        lr: 0.002\n",
    "        vf_loss_coeff: 1\n",
    "        model:\n",
    "            type: \"fully_connected\"\n",
    "            fc_dims: [256, 256]\n",
    "            model_ckpt_filepath: \"\"\n",
    "\n",
    "# Checkpoint saving setting\n",
    "saving:\n",
    "    metrics_log_freq: 100 # how often (in iterations) to print the metrics\n",
    "    model_params_save_freq: 5000 # how often (in iterations) to save the model parameters\n",
    "    basedir: \"/tmp\" # base folder used for saving\n",
    "    name: \"collective_v0\"\n",
    "    tag: \"50preys_1predator\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run_config = yaml.safe_load(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfcdb1a-cf8c-4528-9881-f14adf8f51f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/warpdrive_env/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_manager: Setting Numba to use CUDA device 0\n"
     ]
    }
   ],
   "source": [
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "from custom_env import CUDACustomEnv\n",
    "\n",
    "env_registrar = EnvironmentRegistrar()\n",
    "env_registrar.add_cuda_env_src_path(CUDACustomEnv.name, \"custom_env_step_numba\", env_backend=\"numba\")\n",
    "\n",
    "env_wrapper = EnvWrapper(\n",
    "    env_obj=CUDACustomEnv(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend=\"numba\",\n",
    "    env_registrar=env_registrar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f26aae-eb8a-463e-ba4e-5893152657f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_tag_to_agent_id_map = {\n",
    "    \"predator\": list(env_wrapper.env.predators),\n",
    "    \"prey\": list(env_wrapper.env.preys),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea77e128-653f-4314-9d55-97df4a3f7df3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy module FullyConnected loaded from warp_drive.training.models.fully_connected\n",
      "Policy module FullyConnected loaded from warp_drive.training.models.fully_connected\n"
     ]
    }
   ],
   "source": [
    "import warp_drive.training.trainer\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from importlib import reload\n",
    "reload(warp_drive.training.trainer)\n",
    "trainer = Trainer(\n",
    "    env_wrapper=env_wrapper,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    num_devices=torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cc58683-728f-4ac2-b9b4-0438896a4b56",
   "metadata": {},
   "source": [
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c54822-be12-4932-a31a-64eca7440353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 1 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     367.69\n",
      "Mean action sample time per iter (ms)   :      51.73\n",
      "Mean env. step time per iter (ms)       :     211.82\n",
      "Mean training time per iter (ms)        :     178.67\n",
      "Mean total time per iter (ms)           :     825.83\n",
      "Mean steps per sec (policy eval)        :   27196.61\n",
      "Mean steps per sec (action sample)      :  193312.80\n",
      "Mean steps per sec (env. step)          :   47210.13\n",
      "Mean steps per sec (training time)      :   55969.88\n",
      "Mean steps per sec (total)              :   12109.09\n",
      "========================================\n",
      "Metrics for policy 'prey'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.50000\n",
      "Total loss                              :  187.24503\n",
      "Policy loss                             :   49.55555\n",
      "Value function loss                     :  140.08598\n",
      "Mean rewards                            :    0.99992\n",
      "Max. rewards                            :    1.00000\n",
      "Min. rewards                            :    0.90000\n",
      "Mean value function                     :    0.02996\n",
      "Mean advantages                         :   10.33945\n",
      "Mean (norm.) advantages                 :   10.33945\n",
      "Mean (discounted) returns               :   10.36941\n",
      "Mean normalized returns                 :   10.36941\n",
      "Mean entropy                            :    4.79302\n",
      "Variance explained by the value function:    0.00010\n",
      "Std. of action_0 over agents            :    3.15638\n",
      "Std. of action_0 over envs              :    3.16230\n",
      "Std. of action_0 over time              :    3.15080\n",
      "Std. of action_1 over agents            :    3.10327\n",
      "Std. of action_1 over envs              :    3.10916\n",
      "Std. of action_1 over time              :    3.09569\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    :    0.00000\n",
      "========================================\n",
      "Metrics for policy 'predator'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   90.52756\n",
      "Policy loss                             :  -49.60484\n",
      "Value function loss                     :  140.37199\n",
      "Mean rewards                            :   -1.00000\n",
      "Max. rewards                            :   -1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :    0.01947\n",
      "Mean advantages                         :  -10.34981\n",
      "Mean (norm.) advantages                 :  -10.34981\n",
      "Mean (discounted) returns               :  -10.33034\n",
      "Mean normalized returns                 :  -10.33034\n",
      "Mean entropy                            :    4.79194\n",
      "Variance explained by the value function:    0.00011\n",
      "Std. of action_0 over agents            :        nan\n",
      "Std. of action_0 over envs              :    3.17678\n",
      "Std. of action_0 over time              :    3.16799\n",
      "Std. of action_1 over agents            :        nan\n",
      "Std. of action_1 over envs              :    3.15580\n",
      "Std. of action_1 over time              :    3.13863\n",
      "Current timestep                        : 10000.00000\n",
      "Gradient norm                           :    0.00000\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    :    0.00000\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/collective_v0/50preys_1predator/1679099911/results.json' \n",
      "[Device 0]: Saving the 'prey' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679099911/prey_10000.state_dict'. \n",
      "[Device 0]: Saving the 'predator' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679099911/predator_10000.state_dict'. \n",
      "\n",
      "\n",
      "========================================\n",
      "Device: 0\n",
      "Iterations Completed                    : 25 / 25\n",
      "========================================\n",
      "Speed performance stats\n",
      "========================================\n",
      "Mean policy eval time per iter (ms)     :     148.55\n",
      "Mean action sample time per iter (ms)   :      46.11\n",
      "Mean env. step time per iter (ms)       :     210.83\n",
      "Mean training time per iter (ms)        :     139.14\n",
      "Mean total time per iter (ms)           :     551.56\n",
      "Mean steps per sec (policy eval)        :   67315.60\n",
      "Mean steps per sec (action sample)      :  216885.06\n",
      "Mean steps per sec (env. step)          :   47432.16\n",
      "Mean steps per sec (training time)      :   71867.96\n",
      "Mean steps per sec (total)              :   18130.38\n",
      "========================================\n",
      "Metrics for policy 'prey'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.44600\n",
      "Total loss                              :  530.85687\n",
      "Policy loss                             :   -0.00001\n",
      "Value function loss                     :  530.85687\n",
      "Mean rewards                            :    0.90000\n",
      "Max. rewards                            :    0.90000\n",
      "Min. rewards                            :    0.90000\n",
      "Mean value function                     :  140.73726\n",
      "Mean advantages                         :  -19.77326\n",
      "Mean (norm.) advantages                 :  -19.77326\n",
      "Mean (discounted) returns               :  120.96400\n",
      "Mean normalized returns                 :  120.96400\n",
      "Mean entropy                            :    0.00000\n",
      "Variance explained by the value function:    0.57115\n",
      "Std. of action_0 over agents            :    0.00000\n",
      "Std. of action_0 over envs              :    0.00000\n",
      "Std. of action_0 over time              :    0.00000\n",
      "Std. of action_1 over agents            :    0.00000\n",
      "Std. of action_1 over envs              :    0.00000\n",
      "Std. of action_1 over time              :    0.00000\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.67793\n",
      "Learning rate                           :    0.00500\n",
      "Mean episodic reward                    : 216833.24112\n",
      "========================================\n",
      "Metrics for policy 'predator'\n",
      "========================================\n",
      "VF loss coefficient                     :    1.00000\n",
      "Entropy coefficient                     :    0.05000\n",
      "Total loss                              :   21.63824\n",
      "Policy loss                             :    1.08084\n",
      "Value function loss                     :   20.57161\n",
      "Mean rewards                            :   -1.00000\n",
      "Max. rewards                            :   -1.00000\n",
      "Min. rewards                            :   -1.00000\n",
      "Mean value function                     :  -68.60990\n",
      "Mean advantages                         :    3.82321\n",
      "Mean (norm.) advantages                 :    3.82321\n",
      "Mean (discounted) returns               :  -64.78668\n",
      "Mean normalized returns                 :  -64.78668\n",
      "Mean entropy                            :    0.28424\n",
      "Variance explained by the value function:    0.08619\n",
      "Std. of action_0 over agents            :        nan\n",
      "Std. of action_0 over envs              :    0.81703\n",
      "Std. of action_0 over time              :    0.75194\n",
      "Std. of action_1 over agents            :        nan\n",
      "Std. of action_1 over envs              :    0.00000\n",
      "Std. of action_1 over time              :    0.00000\n",
      "Current timestep                        : 250000.00000\n",
      "Gradient norm                           :    0.68775\n",
      "Learning rate                           :    0.00200\n",
      "Mean episodic reward                    : -4541.73598\n",
      "======================================== \n",
      "\n",
      "[Device 0]: Saving the results to the file '/tmp/collective_v0/50preys_1predator/1679099911/results.json' \n",
      "[Device 0]: Saving the 'prey' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679099911/prey_250000.state_dict'. \n",
      "[Device 0]: Saving the 'predator' torch model to the file: '/tmp/collective_v0/50preys_1predator/1679099911/predator_250000.state_dict'. \n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c77256-da0b-4509-9076-66a36ce7a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device 0]: Loading the provided trainer model checkpoints. \n",
      "[Device 0]: Loading the 'prey' torch model from the previously saved checkpoint: '/tmp/collective_v0/50preys_1predator/1679099911/prey_250000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'prey' model to 250000. \n",
      "[Device 0]: Loading the 'predator' torch model from the previously saved checkpoint: '/tmp/collective_v0/50preys_1predator/1679099911/predator_250000.state_dict' \n",
      "[Device 0]: Updating the timestep for the 'predator' model to 250000. \n"
     ]
    }
   ],
   "source": [
    "trainer.load_model_checkpoint(\n",
    "    {\n",
    "        \"prey\": \"/tmp/collective_v0/50preys_1predator/1679099911/prey_250000.state_dict\",\n",
    "        \"predator\": \"/tmp/collective_v0/50preys_1predator/1679099911/predator_250000.state_dict\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73d560e1-f455-4d3a-bf58-94231d51f90f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize the entire episode roll-out\n",
    "anim = generate_tag_env_rollout_animation(trainer)\n",
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warpdrive-env",
   "language": "python",
   "name": "warpdrive-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
