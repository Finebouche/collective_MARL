{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2d888a-c6df-48c0-b4f9-e0f854f32264",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/warpdrive_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "78abbc29-8a2b-46e8-8889-ff891df34474",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "source": [
    "pip show rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69506557-0774-4887-a043-c60fe9bc88a7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warp_drive.env_wrapper import EnvWrapper\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "from example_envs.tag_continuous.generate_rollout_animation import (\n",
    "    generate_tag_env_rollout_animation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae374982-1ba9-4db5-aeee-0622ac7aec11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from IPython.display import HTML\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29607d4-55c0-4799-802d-1e620514d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3dea70-9d81-4397-b9c7-d6f05fbe029e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the run config.\n",
    "\n",
    "# Here we show an example configures\n",
    "\n",
    "CFG = \"\"\"\n",
    "# Sample YAML configuration for the tag continuous environment\n",
    "name: \"tag_continuous\"\n",
    "\n",
    "# Environment settings\n",
    "env:\n",
    "    num_preys: 50\n",
    "    num_predators: 1\n",
    "    stage_size: 30\n",
    "    episode_length: 500\n",
    "    preparation_length: 100\n",
    "    max_acceleration: 0.1\n",
    "    max_turn: 2.35  # 3*pi/4 radians\n",
    "    num_acceleration_levels: 10\n",
    "    num_turn_levels: 10\n",
    "    eating_reward_for_predator: 10.0\n",
    "    eating_penalty_for_prey: -10.0\n",
    "    edge_hit_penalty: -0.0\n",
    "    end_of_game_penalty : -1.0\n",
    "    end_of_game_reward: 1.0\n",
    "    use_full_observation: False\n",
    "    eating_distance: 0.02\n",
    "    seed: 274880\n",
    "    env_backend: \"numba\"\n",
    "\n",
    "# Trainer settings\n",
    "trainer:\n",
    "    num_envs: 400 # number of environment replicas\n",
    "    train_batch_size: 10000 # total batch size used for training per iteration (across all the environments)\n",
    "    num_episodes: 500 # number of episodes to run the training for (can be arbitrarily high)\n",
    "# Policy network settings\n",
    "policy: # list all the policies below\n",
    "    prey:\n",
    "        to_train: True # flag indicating whether the model needs to be trained\n",
    "        algorithm: \"A2C\" # algorithm used to train the policy\n",
    "        gamma: 0.98 # discount rate gamms\n",
    "        lr: 0.005 # learning rate\n",
    "        vf_loss_coeff: 1 # loss coefficient for the value function loss\n",
    "        entropy_coeff:\n",
    "        - [0, 0.5]\n",
    "        - [2000000, 0.05]\n",
    "        model: # policy model settings\n",
    "            type: \"fully_connected\" # model type\n",
    "            fc_dims: [256, 256] # dimension(s) of the fully connected layers as a list\n",
    "            model_ckpt_filepath: \"\" # filepath (used to restore a previously saved model)\n",
    "    predator:\n",
    "        to_train: True\n",
    "        algorithm: \"A2C\"\n",
    "        gamma: 0.98\n",
    "        lr: 0.002\n",
    "        vf_loss_coeff: 1\n",
    "        model:\n",
    "            type: \"fully_connected\"\n",
    "            fc_dims: [256, 256]\n",
    "            model_ckpt_filepath: \"\"\n",
    "\n",
    "# Checkpoint saving setting\n",
    "saving:\n",
    "    metrics_log_freq: 100 # how often (in iterations) to print the metrics\n",
    "    model_params_save_freq: 5000 # how often (in iterations) to save the model parameters\n",
    "    basedir: \"/tmp\" # base folder used for saving\n",
    "    name: \"collective_v0\"\n",
    "    tag: \"50preys_1predator\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run_config = yaml.safe_load(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1092e512-dc56-4bc6-bbab-235f86ef0b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from custom_env import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfcdb1a-cf8c-4528-9881-f14adf8f51f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pycuda': {}, 'numba': {'customenv': 'custom_env_step_numba'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "from custom_env import CustomEnv\n",
    "\n",
    "env_registrar = EnvironmentRegistrar()\n",
    "env_registrar.add_cuda_env_src_path(CustomEnv.name, \"custom_env_step_numba\", env_backend=\"numba\")\n",
    "\n",
    "env_registrar._customized_cuda_env_src_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f30a4c2-3adc-4efd-b05c-17d2fba7e18e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/warpdrive_env/lib/python3.8/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_manager: Setting Numba to use CUDA device 0\n"
     ]
    }
   ],
   "source": [
    "env_wrapper = EnvWrapper(\n",
    "    env_obj=CustomEnv(**run_config[\"env\"]),\n",
    "    num_envs=run_config[\"trainer\"][\"num_envs\"],\n",
    "    env_backend=\"numba\",\n",
    "    env_registrar=env_registrar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a441434-c574-4bd6-8231-55b67e499c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing the consistency checks for scenario: test1...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 3 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m      4\u001b[0m env_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_preys\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_predators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m testing_class \u001b[38;5;241m=\u001b[39m EnvironmentCPUvsGPU(\n\u001b[1;32m     12\u001b[0m     dual_mode_env_class\u001b[38;5;241m=\u001b[39mCustomEnv,\n\u001b[1;32m     13\u001b[0m     env_configs\u001b[38;5;241m=\u001b[39menv_configs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     env_registrar\u001b[38;5;241m=\u001b[39menv_registrar,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtesting_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_env_reset_and_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/env_cpu_gpu_consistency_checker.py:201\u001b[0m, in \u001b[0;36mEnvironmentCPUvsGPU.test_env_reset_and_step\u001b[0;34m(self, consistency_threshold_pct, seed)\u001b[0m\n\u001b[1;32m    197\u001b[0m     env_cpu[env_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_registrar\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_env_class\u001b[38;5;241m.\u001b[39mname, env_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_config)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     env_cpu[env_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_env_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu_testing_mode:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m env_cpu[env_id]\u001b[38;5;241m.\u001b[39mn_agents \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/utils/argument_fix.py:46\u001b[0m, in \u001b[0;36mArgfix.__call__.<locals>.fixed_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         kwargs[new_arg] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_arg)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Call the function with the fixed arguments\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/env_wrapper.py:107\u001b[0m, in \u001b[0;36mEnvWrapper.__init__\u001b[0;34m(self, env_obj, env_name, env_config, num_envs, blocks_per_env, env_backend, testing_mode, testing_bin_filename, env_registrar, event_messenger, process_id)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Add observation space to the env\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_at_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m recursive_obs_dict_to_spaces_dict(obs)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Ensure the observation and action spaces share the same keys\u001b[39;00m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/env_wrapper.py:366\u001b[0m, in \u001b[0;36mEnvWrapper.obs_at_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobs_at_reset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Calls the (Python) env to reset and return the initial state\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project_ghent/collective_MARL/custom_env.py:535\u001b[0m, in \u001b[0;36mCustomEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreys \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreys_at_reset)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_preys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreys)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project_ghent/collective_MARL/custom_env.py:356\u001b[0m, in \u001b[0;36mCustomEnv.generate_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m     is_visible \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_distance(agent_id, observed_agent_id) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seeing_distance \u001b[38;5;28;01mfor\u001b[39;00m observed_agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents)])\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstill_in_the_game[agent_id] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_full_observation:\n\u001b[1;32m    354\u001b[0m         obs[agent_id] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m    355\u001b[0m             [\n\u001b[0;32m--> 356\u001b[0m                 \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnormalized_global_obs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalized_global_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m                    \u001b[49m\u001b[43magent_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstill_in_the_game\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mis_visible\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:,[idx \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents) \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m agent_id],]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;66;03m# filter out the obs for the current agent\u001b[39;00m\n\u001b[1;32m    362\u001b[0m                 time,\n\u001b[1;32m    363\u001b[0m             ]\n\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Set to 0\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# obs = [global_obs, agent_types, still_in_the_game, 0]\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     obs[agent_id] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m    369\u001b[0m         [\n\u001b[1;32m    370\u001b[0m             np\u001b[38;5;241m.\u001b[39mvstack((\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m         ]\n\u001b[1;32m    378\u001b[0m     )\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 3 has size 1"
     ]
    }
   ],
   "source": [
    "from warp_drive.env_cpu_gpu_consistency_checker import EnvironmentCPUvsGPU\n",
    "\n",
    "\n",
    "env_configs = {\n",
    "    \"test1\": {\n",
    "        \"num_preys\": 4,\n",
    "        \"num_predators\": 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "testing_class = EnvironmentCPUvsGPU(\n",
    "    dual_mode_env_class=CustomEnv,\n",
    "    env_configs=env_configs,\n",
    "    num_envs=2,\n",
    "    num_episodes=2,\n",
    "    env_registrar=env_registrar,\n",
    ")\n",
    "\n",
    "testing_class.test_env_reset_and_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f26aae-eb8a-463e-ba4e-5893152657f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_tag_to_agent_id_map = {\n",
    "    \"predator\": list(env_wrapper.env.predators),\n",
    "    \"prey\": list(env_wrapper.env.preys),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea77e128-653f-4314-9d55-97df4a3f7df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomEnv' object has no attribute 'eating_reward_for_predator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_tag_to_agent_id_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/training/trainer.py:212\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, env_wrapper, config, policy_tag_to_agent_id_map, create_separate_placeholders_for_each_policy, obs_dim_corresponding_to_num_agents, num_devices, device_id, results_dir, verbose)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_batch_size_per_env \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Push all the data and tensor arrays to the GPU\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# upon resetting environments for the very first time.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_envs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_all_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_wrapper\u001b[38;5;241m.\u001b[39menv_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpycuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarp_drive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanagers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpycuda_managers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpycuda_function_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    216\u001b[0m         PyCUDASampler,\n\u001b[1;32m    217\u001b[0m     )\n",
      "File \u001b[0;32m/project_ghent/warpdrive_env/lib/python3.8/site-packages/warp_drive/env_wrapper.py:295\u001b[0m, in \u001b[0;36mEnvWrapper.reset_all_envs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack([array \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_envs)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Copy host data and tensors to device\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Note: this happens only once after the first reset on the host\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Add env dimension to data if \"save_copy_and_apply_at_reset\" is True\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m data_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m tensor_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_tensor_dictionary()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data_dictionary:\n",
      "File \u001b[0;32m/project_ghent/collective_MARL/custom_env.py:475\u001b[0m, in \u001b[0;36mCustomEnv.get_data_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m data_dict\u001b[38;5;241m.\u001b[39madd_data(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_full_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_full_observation)\n\u001b[1;32m    471\u001b[0m data_dict\u001b[38;5;241m.\u001b[39madd_data(\n\u001b[1;32m    472\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_margin_for_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_margin_for_reward\n\u001b[1;32m    473\u001b[0m )\n\u001b[1;32m    474\u001b[0m data_dict\u001b[38;5;241m.\u001b[39madd_data(\n\u001b[0;32m--> 475\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meating_reward_for_predator\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meating_reward_for_predator\u001b[49m\n\u001b[1;32m    476\u001b[0m )\n\u001b[1;32m    477\u001b[0m data_dict\u001b[38;5;241m.\u001b[39madd_data(\n\u001b[1;32m    478\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meating_penalty_for_prey\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meating_penalty_for_prey\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    480\u001b[0m data_dict\u001b[38;5;241m.\u001b[39madd_data(\n\u001b[1;32m    481\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_of_game_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    482\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_game_penalty,\n\u001b[1;32m    483\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomEnv' object has no attribute 'eating_reward_for_predator'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(env_wrapper, run_config, policy_tag_to_agent_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30f14d-8670-4f7c-9a01-e6fde304a2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warpdrive-env",
   "language": "python",
   "name": "warpdrive-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
